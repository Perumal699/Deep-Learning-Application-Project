{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "2)Use the data file\"yelp_labelled.txt\".The file has two columns.The first column is the text sentence and the second column is Score. Score is either 1(for positive) or 0 (for negative)\n",
        "\n",
        "a. find out how many total words or features exist in the corpus?"
      ],
      "metadata": {
        "id": "rYQFANxkR7Fv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3ediUgpQvTM",
        "outputId": "db6b093a-77a2-4862-8b96-10bd00b47097"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words or features in the corpus: 2969\n"
          ]
        }
      ],
      "source": [
        "\n",
        "with open(\"yelp_labelled.txt\", \"r\") as file:\n",
        "    lines = file.readlines()\n",
        "unique_words = set()\n",
        "for line in lines:\n",
        "\n",
        "    text, score = line.strip().split('\\t')\n",
        "    words = text.split()\n",
        "    unique_words.update(words)\n",
        "\n",
        "\n",
        "total_words = len(unique_words)\n",
        "\n",
        "print(\"Total words or features in the corpus:\", total_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "b.Which are the 10 words with the highest frequency or total count?"
      ],
      "metadata": {
        "id": "Pud90_PjSM9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "with open(\"yelp_labelled.txt\", \"r\") as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "word_counter = Counter()\n",
        "for line in lines:\n",
        "    text, score = line.strip().split('\\t')\n",
        "    word_counter.update(text.split())\n",
        "\n",
        "top_10_words = word_counter.most_common(10)\n",
        "\n",
        "print(\"Top 10 words with highest frequency:\")\n",
        "for word, count in top_10_words:\n",
        "    print(word, \"-\", count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVhIDJvSSBpD",
        "outputId": "68e7f37e-c6e0-4277-c52a-51288043a303"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 words with highest frequency:\n",
            "the - 405\n",
            "and - 378\n",
            "I - 294\n",
            "was - 292\n",
            "a - 228\n",
            "to - 213\n",
            "The - 177\n",
            "is - 171\n",
            "of - 123\n",
            "not - 103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "c.How many words or features exist with total frequency less than 3 in the whole corpus?"
      ],
      "metadata": {
        "id": "TkoAnEBXSuIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "with open(\"yelp_labelled.txt\", \"r\") as file:\n",
        "    lines = file.readlines()\n",
        "word_counter = Counter()\n",
        "for line in lines:\n",
        "    text, score = line.strip().split('\\t')\n",
        "    word_counter.update(text.split())\n",
        "words_less_than_3 = sum(1 for word, count in word_counter.items() if count < 3)\n",
        "\n",
        "print(\"Number of words or features with total frequency less than 3:\", words_less_than_3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBIhoegfTLhR",
        "outputId": "2b81b9b6-169a-4bec-e563-c7d6c99ed668"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words or features with total frequency less than 3: 2373\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "d.Find the parts of speech of all words"
      ],
      "metadata": {
        "id": "LWFe_n9QTWhv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "with open(\"yelp_labelled.txt\", \"r\") as file:\n",
        "    lines = file.readlines()\n",
        "all_pos_tags = []\n",
        "\n",
        "for line in lines:\n",
        "    text, score = line.strip().split('\\t')\n",
        "\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    pos_tags = pos_tag(words)\n",
        "\n",
        "    all_pos_tags.extend(pos_tags)\n",
        "\n",
        "print(\"Parts of speech of all words:\")\n",
        "for word, pos in all_pos_tags:\n",
        "    print(word, \"-\", pos)\n"
      ],
      "metadata": {
        "id": "QkW8aJ_cTn1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "e. Build a suitable classification model"
      ],
      "metadata": {
        "id": "y-PVjHOzUsp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# 1 Data Preprocessing\n",
        "df = pd.read_csv(\"yelp_labelled.txt\", sep='\\t', header=None)\n",
        "X = df[0]  # Features\n",
        "y = df[1]  # Target labels\n",
        "\n",
        "#2:Feature Engineering (TF-IDF Vectorization)\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # Adjust max_features as needed\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
        "\n",
        "#3:Model Selection and Training\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "classifier = MultinomialNB()  # Naive Bayes classifier\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "#4:Model Evaluation\n",
        "y_pred = classifier.predict(X_val)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_val, y_pred))\n",
        "\n",
        "# 5: Hyperparameter Tuning (Optional)\n",
        "# 6: Model Deployment (Optional)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VEMtvTyUbvw",
        "outputId": "6d68be1a-5b73-45dd-b971-6f5e25924360"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.84      0.80        96\n",
            "           1       0.84      0.75      0.79       104\n",
            "\n",
            "    accuracy                           0.80       200\n",
            "   macro avg       0.80      0.80      0.79       200\n",
            "weighted avg       0.80      0.80      0.79       200\n",
            "\n"
          ]
        }
      ]
    }
  ]
}